{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9243,"sourceType":"datasetVersion","datasetId":2243}],"dockerImageVersionId":30715,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Importar las librerias necesarias\nimport helper\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom timeit import default_timer as timer\nfrom tqdm.auto import tqdm\n\nimport torch \nfrom torch import nn\nfrom torchvision import datasets, transforms, utils\nfrom torch.utils.data import Dataset, DataLoader\nfrom ignite.handlers.param_scheduler import create_lr_scheduler_with_warmup\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-09T18:26:44.212165Z","iopub.execute_input":"2024-06-09T18:26:44.212985Z","iopub.status.idle":"2024-06-09T18:26:44.219328Z","shell.execute_reply.started":"2024-06-09T18:26:44.212955Z","shell.execute_reply":"2024-06-09T18:26:44.218352Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Utilizar GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:26:44.221402Z","iopub.execute_input":"2024-06-09T18:26:44.221857Z","iopub.status.idle":"2024-06-09T18:26:44.230761Z","shell.execute_reply.started":"2024-06-09T18:26:44.221825Z","shell.execute_reply":"2024-06-09T18:26:44.229988Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **PREPARACIÓN DATASETS**","metadata":{}},{"cell_type":"code","source":"#NORMALIZAR INPUTS modelo 1 y 2\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0,), (1,))])\n\n# #DESCARGAR DATASETS\ntrain_set = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform = transform)\n\ntest_set = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform = transform)\n\n\n# #CREAR DATALOADERS\nbatch_size = 32\n\ntrainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\ntestloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:26:44.231838Z","iopub.execute_input":"2024-06-09T18:26:44.232179Z","iopub.status.idle":"2024-06-09T18:26:44.319447Z","shell.execute_reply.started":"2024-06-09T18:26:44.232148Z","shell.execute_reply":"2024-06-09T18:26:44.318668Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **EXPLORACIÓN DE LOS DATASETS**","metadata":{}},{"cell_type":"code","source":"##Informacion del dataset\n\ntrain_dataiter = iter(trainloader)\ntrain_images, train_labels = next(train_dataiter)\n\ntest_dataiter = iter(testloader)\ntest_images, test_labels = next(test_dataiter)\n\nprint(\"Training set\")\nprint(\"\")\nprint(train_set)\nprint(\"\")\nprint(\"Dimensiones mini-batch\")\nprint(train_images.shape)\nprint(train_labels.shape)\n\nprint(\"\")\nprint(\"\")\nprint(\"Test set\")\nprint(\"\")\nprint(test_set)\nprint(\"\")\nprint(\"Dimensiones mini-batch\")\nprint(test_images.shape)\nprint(test_labels.shape)\n\n#Dataloaders info\nprint(\"\")\nprint(\"Dataloaders lenght:\")\nprint(f\"trainloader: {len(trainloader)} batches\")\nprint(f\"testloader: {len(testloader)} batches\")\n\n#Clases\nprint(\"\")\nprint(\"------------CLASES------------\")\nclasses = train_set.class_to_idx\nclassNames = train_set.classes\nclasses","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:26:44.321572Z","iopub.execute_input":"2024-06-09T18:26:44.321864Z","iopub.status.idle":"2024-06-09T18:26:44.358937Z","shell.execute_reply.started":"2024-06-09T18:26:44.321840Z","shell.execute_reply":"2024-06-09T18:26:44.358176Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Visualizar imagenes del set\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 2, 2\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(0, len(train_set), size=[1]).item() #numero aleatorio\n    img, label = train_set[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(classNames[label] + f\", label: {label}\")\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:26:44.360066Z","iopub.execute_input":"2024-06-09T18:26:44.360344Z","iopub.status.idle":"2024-06-09T18:26:44.757231Z","shell.execute_reply.started":"2024-06-09T18:26:44.360321Z","shell.execute_reply":"2024-06-09T18:26:44.756266Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Funcion precision\n\ndef accuracy_fn(y_true, y_pred):\n    correct = torch.eq(y_true, y_pred).sum().item()\n    accuracy = (correct / len(y_pred)) * 100\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:26:44.758579Z","iopub.execute_input":"2024-06-09T18:26:44.758882Z","iopub.status.idle":"2024-06-09T18:26:44.764148Z","shell.execute_reply.started":"2024-06-09T18:26:44.758857Z","shell.execute_reply":"2024-06-09T18:26:44.763197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#TRAIN AND TEST STEP (iteraciones)\n\ndef train_step(model, dataloader, optimizer, loss_fn, accuracy_fn, device, scheduler):\n    \n    train_loss, train_acc = 0,0\n    \n    model.to(device)\n    \n    for batch, (X,y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n        \n        #forward pass\n        y_pred = model(X)\n        \n        #Perdida per batch\n        loss = loss_fn(y_pred,y)\n        train_loss += loss #acumula la perdida\n        train_acc += accuracy_fn(y_true = y, y_pred =y_pred.argmax(dim=1))\n        \n        #optimizador gradientes acumulados a cero\n        optimizer.zero_grad()\n        \n        loss.backward()\n            \n        optimizer.step()\n        \n        if batch % 256 == 0:\n            print(f\"Revisados {batch*len(X)}/{len(dataloader.dataset)} samples\")\n        \n    #perdida y precision promedio por batch\n    train_loss /= len(dataloader)\n    train_acc /= len(dataloader)\n    \n    print(\"\")\n    print(f\"Train Loss {train_loss:.5f} | Train Acc: {train_acc:.5f}%\")\n    \n    if scheduler != None:\n        print(\"LR antes: \" + str(optimizer.param_groups[0]['lr']))\n        scheduler(None) #actualizar LR\n        print(\"LR despues: \" + str(optimizer.param_groups[0]['lr']))\n        \n    return train_loss,train_acc\n    \ndef test_step(model, dataloader,loss_fn, accuracy_fn, device):\n    \n    test_loss, test_acc = 0,0\n    \n    model.to(device)\n    \n    model.eval() #poner el modelo en modo evaluacion\n    \n    #context manager\n    with torch.inference_mode():\n        for X,y in dataloader:\n            X, y = X.to(device), y.to(device)\n            \n            #forward pass\n            test_pred = model(X)\n            \n            #perdida y precision\n            test_loss += loss_fn(test_pred, y)\n            test_acc += accuracy_fn(y_true = y, y_pred=test_pred.argmax(dim=1))\n            \n        #perdida y precision promedio por batch\n        test_loss /= len(dataloader)\n        test_acc /= len(dataloader)\n        \n        print(\"\")\n        print(f\"Test Loss {test_loss:.5f} | Test Acc: {test_acc:.5f}%\")\n        \n    return test_loss,test_acc","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:26:44.765436Z","iopub.execute_input":"2024-06-09T18:26:44.765770Z","iopub.status.idle":"2024-06-09T18:26:44.779714Z","shell.execute_reply.started":"2024-06-09T18:26:44.765721Z","shell.execute_reply":"2024-06-09T18:26:44.778849Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Función para correr el modelo\ndef run_model(model, trainloader, testloader, loss_function, optimizer, accuracy_fn, device, epochs, scheduler):\n    train_time_start_model = timer() #captar tiempo inicial\n    \n    epoch_train_loss = []\n    epoch_train_acc = []\n    epoch_test_loss = []\n    epoch_test_acc = []\n    \n    for epoch in tqdm(range(epochs)):\n        print(f\"Epoch: {epoch+1}\\n\")\n\n        epoch_loss, epoch_acc = train_step(model = model,dataloader=trainloader,loss_fn=loss_function,optimizer=optimizer, accuracy_fn=accuracy_fn, device=device, scheduler=scheduler)\n        epoch_train_loss.append(epoch_loss)\n        epoch_train_acc.append(epoch_acc)\n        \n        epoch_loss, epoch_acc = test_step(model=model, dataloader=testloader,loss_fn=loss_function,accuracy_fn=accuracy_fn, device=device)\n        epoch_test_loss.append(epoch_loss)\n        epoch_test_acc.append(epoch_acc)\n        \n    train_time_end_model = timer() #captar tiempo final\n    total_time = train_time_end_model - train_time_start_model\n    print(f\"Tiempo de entrenamiento en {str(device)}: {total_time:.3f} segundos\")\n    \n    return  epoch_train_loss,epoch_train_acc,epoch_test_loss,epoch_test_acc ","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:26:44.781065Z","iopub.execute_input":"2024-06-09T18:26:44.781443Z","iopub.status.idle":"2024-06-09T18:26:44.793276Z","shell.execute_reply.started":"2024-06-09T18:26:44.781410Z","shell.execute_reply":"2024-06-09T18:26:44.792425Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#EVALUAR EL MODELO\ndef evaluate_model(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_function: torch.nn.Module, accuracy_fn, device=device):\n    loss, acc = 0,0\n    model.eval()\n    with torch.inference_mode():\n        \n        for X, y in tqdm(data_loader):\n            X, y = X.to(device), y.to(device)\n            y_pred = model(X)\n            \n            loss += loss_function(y_pred, y)\n            acc += accuracy_fn(y, y_pred.argmax(dim=1))\n            \n        loss /= len(data_loader)\n        acc /= len(data_loader)\n        \n    return {\"model_name\": model.__class__.__name__,\"model_loss\": loss.item(),\"model_acc\": acc}\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:26:44.794459Z","iopub.execute_input":"2024-06-09T18:26:44.794831Z","iopub.status.idle":"2024-06-09T18:26:44.802946Z","shell.execute_reply.started":"2024-06-09T18:26:44.794801Z","shell.execute_reply":"2024-06-09T18:26:44.802110Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Graficar funcion de perdida y de precision\n\ndef plot_loss(epoch_train_loss, epoch_test_loss):\n    \n    \n    epoch_train_loss_temp = [x.cpu().detach().numpy() for x in epoch_train_loss]\n\n    epoch_test_loss_temp = [x.cpu().detach().numpy() for x in epoch_test_loss]\n\n    plt.plot(np.arange(len(epoch_train_loss_temp)), epoch_train_loss_temp, 'r', label = \"Training loss\")\n    plt.plot(np.arange(len(epoch_test_loss_temp)), epoch_test_loss_temp, 'b', label = \"Test loss\")\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n    \ndef plot_acc(epoch_train_acc, epoch_test_acc):\n    plt.plot(np.arange(len(epoch_train_acc)), epoch_train_acc, 'r', label=\"Training accuracy\")\n    plt.plot(np.arange(len(epoch_test_acc)), epoch_test_acc, 'b', label=\"Test accuracy\")\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:26:44.806279Z","iopub.execute_input":"2024-06-09T18:26:44.806546Z","iopub.status.idle":"2024-06-09T18:26:44.818355Z","shell.execute_reply.started":"2024-06-09T18:26:44.806524Z","shell.execute_reply":"2024-06-09T18:26:44.817544Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **ARQUITECTURA 1**","metadata":{}},{"cell_type":"code","source":"#MODELO 1\n\nclass ModelV1(nn.Module):\n    def __init__(self, input_shape, hidden_units, output_shape):\n        super().__init__()\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels = input_shape,\n                     out_channels = hidden_units,\n                     kernel_size=3,\n                     stride=1,\n                     padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2))\n        \n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(in_channels = hidden_units,\n                     out_channels = hidden_units,\n                     kernel_size=3,\n                     stride=1,\n                     padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = hidden_units,\n                     out_channels = hidden_units,\n                     kernel_size=3,\n                     stride=1,\n                     padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2)\n            )\n        \n        self.conv_block_3 = nn.Sequential(\n            nn.Conv2d(in_channels = hidden_units,\n                     out_channels = hidden_units,\n                     kernel_size=3,\n                     stride=1,\n                     padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels = hidden_units,\n                     out_channels = hidden_units,\n                     kernel_size=3,\n                     stride=1,\n                     padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2)\n            )      \n\n        \n        #fully connected\n        \n        self.classifier = nn.Sequential(\n            nn.Flatten(),#transformar output en un solo vector\n            nn.Linear(in_features=hidden_units*9,\n                    out_features = output_shape),\n            nn.LogSoftmax(1)\n            )\n        \n    \n    def forward(self, x):\n        x= self.conv_block_1(x)\n        #print(x.shape) \n        x= self.conv_block_2(x)\n        #print(x.shape) evaluar dimensiones del output del bloque convolucional al inicio\n        x= self.conv_block_3(x)\n        #print(x.shape)\n        x = self.classifier(x)\n        #print(x.shape)\n        #print(x)\n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:26:44.819277Z","iopub.execute_input":"2024-06-09T18:26:44.819508Z","iopub.status.idle":"2024-06-09T18:26:44.830748Z","shell.execute_reply.started":"2024-06-09T18:26:44.819488Z","shell.execute_reply":"2024-06-09T18:26:44.829890Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Hiperparámetros modelo 1 (88.75)\nmodel_1 = ModelV1(input_shape = 1,\n                hidden_units = 15,\n               output_shape = len(classNames))\n\nloss_function = nn.NLLLoss()\noptimizer = torch.optim.SGD(params = model_1.parameters(), lr=0.01, momentum=0.9)\nepochs = 6\n\n#EJECUTAR MODELO\nepoch_train_loss,epoch_train_acc,epoch_test_loss,epoch_test_acc = run_model(model_1, trainloader, testloader, loss_function, optimizer, accuracy_fn, device, epochs, scheduler = None)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:26:44.831885Z","iopub.execute_input":"2024-06-09T18:26:44.832226Z","iopub.status.idle":"2024-06-09T18:28:45.720983Z","shell.execute_reply.started":"2024-06-09T18:26:44.832197Z","shell.execute_reply":"2024-06-09T18:28:45.720052Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#GRAFICAR LOSS Y ACC\nplot_loss(epoch_train_loss,epoch_test_loss)\n\nplot_acc(epoch_train_acc,epoch_test_acc)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:28:45.768963Z","iopub.execute_input":"2024-06-09T18:28:45.769449Z","iopub.status.idle":"2024-06-09T18:28:46.295370Z","shell.execute_reply.started":"2024-06-09T18:28:45.769424Z","shell.execute_reply":"2024-06-09T18:28:46.294521Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#OBTENER RESULTADOS\nmodel_1_results = evaluate_model(model_1, testloader, loss_function, accuracy_fn, device)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:28:46.296559Z","iopub.execute_input":"2024-06-09T18:28:46.296837Z","iopub.status.idle":"2024-06-09T18:28:48.683993Z","shell.execute_reply.started":"2024-06-09T18:28:46.296814Z","shell.execute_reply":"2024-06-09T18:28:48.683144Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **ARQUITECTURA 2**","metadata":{}},{"cell_type":"code","source":"#inicializacion de xavier (normal)\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        torch.nn.init.xavier_normal_(m.weight)\n        torch.nn.init.zeros_(m.bias)\n        \n        \n#CREAR DATALOADERS V2\nbatch_size = 64\n\ntrainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\ntestloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:28:48.685401Z","iopub.execute_input":"2024-06-09T18:28:48.686073Z","iopub.status.idle":"2024-06-09T18:28:48.692514Z","shell.execute_reply.started":"2024-06-09T18:28:48.686039Z","shell.execute_reply":"2024-06-09T18:28:48.691584Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MODELO 2\nclass ModelV2(nn.Module):\n    def __init__(self, input_shape, hidden_units, output_shape):\n        super().__init__()\n         #inicializar pesos \n        self.apply(weights_init)\n        \n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels = input_shape,\n                     out_channels = hidden_units,\n                     kernel_size=3,\n                     stride=1,\n                     padding=1),\n            nn.BatchNorm2d(hidden_units),\n            nn.LeakyReLU(),\n            nn.MaxPool2d(kernel_size = 2),\n            nn.Dropout2d(p=0.5)\n            )\n            \n        \n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(in_channels = hidden_units,\n                     out_channels = hidden_units,\n                     kernel_size=3,\n                     stride=1,\n                     padding=1),\n            nn.BatchNorm2d(hidden_units),\n            nn.LeakyReLU(),\n            nn.Conv2d(in_channels = hidden_units,\n                     out_channels = hidden_units,\n                     kernel_size=3,\n                     stride=1,\n                     padding=1),\n            nn.BatchNorm2d(hidden_units),\n            nn.LeakyReLU(),\n            nn.MaxPool2d(kernel_size = 2),\n            nn.Dropout2d(p=0.5)\n            )\n        \n        self.conv_block_3 = nn.Sequential(\n            nn.Conv2d(in_channels = hidden_units,\n                     out_channels = hidden_units,\n                     kernel_size=3,\n                     stride=1,\n                     padding=1),\n            nn.BatchNorm2d(hidden_units),\n            nn.LeakyReLU(),\n            nn.Conv2d(in_channels = hidden_units,\n                     out_channels = hidden_units,\n                     kernel_size=3,\n                     stride=1,\n                     padding=1),\n            nn.BatchNorm2d(hidden_units),\n            nn.LeakyReLU(),\n            nn.MaxPool2d(kernel_size = 2),\n            nn.Dropout2d(p=0.5)\n            )\n\n        \n        #fully connected\n        \n        self.classifier = nn.Sequential(\n            nn.Flatten(),#transformar output en un solo vector\n            nn.Linear(in_features=hidden_units*9,\n                    out_features = hidden_units),\n            nn.BatchNorm1d(hidden_units),\n            nn.LogSoftmax(1)\n            )\n        \n    \n    def forward(self, x):\n        x= self.conv_block_1(x)\n        #print(x.shape) \n        x= self.conv_block_2(x)\n        #print(x.shape)\n        x= self.conv_block_3(x)\n        #print(x.shape)\n        x = self.classifier(x)\n        #print(x.shape)\n        #print(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:28:48.694068Z","iopub.execute_input":"2024-06-09T18:28:48.694317Z","iopub.status.idle":"2024-06-09T18:28:48.707642Z","shell.execute_reply.started":"2024-06-09T18:28:48.694296Z","shell.execute_reply":"2024-06-09T18:28:48.706770Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Hiperparámetros modelo 2 (91,28%)\nmodel_2 = ModelV2(input_shape = 1,\n                hidden_units = 15,\n               output_shape = len(classNames))\n\nloss_function = nn.NLLLoss()\nepochs = 10\n\n#LEARNING RATE WARMUP\ninitial_lr = 0.01\nwarmup_iteration = 5\nwarmup_initial_lr = 1e-5\n\noptimizer = torch.optim.Adam(params = model_2.parameters(), lr=initial_lr, betas=(0.9,0.999))\nlr_scheduler = create_lr_scheduler_with_warmup(CosineAnnealingLR(optimizer, T_max=epochs-warmup_iteration),\n                                               warmup_start_value=warmup_initial_lr,\n                                               warmup_duration=warmup_iteration,\n                                               warmup_end_value=initial_lr)\n\n\n\nepoch_train_loss,epoch_train_acc,epoch_test_loss,epoch_test_acc = run_model(model_2, trainloader, testloader, loss_function, optimizer, accuracy_fn, device, epochs, scheduler = lr_scheduler)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:28:48.708733Z","iopub.execute_input":"2024-06-09T18:28:48.708960Z","iopub.status.idle":"2024-06-09T18:31:56.469660Z","shell.execute_reply.started":"2024-06-09T18:28:48.708940Z","shell.execute_reply":"2024-06-09T18:31:56.468758Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#GRAFICAR LOSS Y ACC\nplot_loss(epoch_train_loss,epoch_test_loss)\n\nplot_acc(epoch_train_acc,epoch_test_acc)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:31:56.537103Z","iopub.execute_input":"2024-06-09T18:31:56.537440Z","iopub.status.idle":"2024-06-09T18:31:57.019340Z","shell.execute_reply.started":"2024-06-09T18:31:56.537406Z","shell.execute_reply":"2024-06-09T18:31:57.018480Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #OBTENER RESULTADOS\nmodel_2_results = evaluate_model(model_2, testloader, loss_function, accuracy_fn, device)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:31:57.020649Z","iopub.execute_input":"2024-06-09T18:31:57.021015Z","iopub.status.idle":"2024-06-09T18:31:59.222819Z","shell.execute_reply.started":"2024-06-09T18:31:57.020982Z","shell.execute_reply":"2024-06-09T18:31:59.221893Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **ARQUITECTURA 3**","metadata":{}},{"cell_type":"code","source":"# #ALTERACIÓN DE INPUTS \n\n# #PREPARACIÓN PARA DATA AUGMENTATION\n# transform1 = transforms.Compose([transforms.ToTensor(),    \n#                                 transforms.ColorJitter(brightness=(0.6,1.4), hue=0.2, saturation=(0.6,1.4) ),\n#                                 transforms.RandomHorizontalFlip(p=0.5),\n#                                 transforms.Normalize((0,), (1,))])\n# #NORMALIZACIÓN INPUTS\n# transform2 = transforms.Compose([transforms.ToTensor(),\n#                                 transforms.Normalize((0,), (1,))])\n\n\n# #DESCARGAR DATASETS\n# train_norm = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform = transform2)\n# train_augm = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform = transform1)\n# train_set = torch.utils.data.ConcatDataset([train_norm,train_augm]) #Incrementar el tamaño del trainset al doble\n\n# test_set = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform = transform2)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:31:59.224067Z","iopub.execute_input":"2024-06-09T18:31:59.224339Z","iopub.status.idle":"2024-06-09T18:31:59.229220Z","shell.execute_reply.started":"2024-06-09T18:31:59.224315Z","shell.execute_reply":"2024-06-09T18:31:59.228089Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#inicializacion de xavier (normal)\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        torch.nn.init.xavier_normal_(m.weight)\n        torch.nn.init.zeros_(m.bias)\n        \n        \n#CREAR DATALOADERS V3\nbatch_size = 128\n\ntrainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\ntestloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:31:59.230215Z","iopub.execute_input":"2024-06-09T18:31:59.230476Z","iopub.status.idle":"2024-06-09T18:31:59.242757Z","shell.execute_reply.started":"2024-06-09T18:31:59.230455Z","shell.execute_reply":"2024-06-09T18:31:59.241931Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Modelo 3\nclass ModelV3(nn.Module):\n    def __init__(self, input_shape, hidden_units, output_shape):\n        super().__init__()\n         #inicializar pesos \n        self.apply(weights_init)\n        \n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels = input_shape,\n                     out_channels = hidden_units,\n                     kernel_size=3,\n                     stride=1,\n                     padding=1),\n            nn.BatchNorm2d(hidden_units),\n            nn.LeakyReLU(),\n            nn.MaxPool2d(kernel_size = 2),\n            nn.Dropout2d(p=0.6)\n            )\n            \n        \n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(in_channels = hidden_units,\n                     out_channels = hidden_units,\n                     kernel_size=3,\n                     stride=1,\n                     padding=1),\n            nn.BatchNorm2d(hidden_units),\n            nn.LeakyReLU(),\n            nn.Conv2d(in_channels = hidden_units,\n                     out_channels = hidden_units,\n                     kernel_size=3,\n                     stride=1,\n                     padding=1),\n            nn.BatchNorm2d(hidden_units),\n            nn.LeakyReLU(),\n            nn.MaxPool2d(kernel_size = 2),\n            nn.Dropout2d(p=0.6)\n            )\n        \n        \n        #fully connected\n        \n        self.classifier1 = nn.Sequential(\n            nn.Flatten(),#transformar output en un solo vector\n            nn.Linear(in_features=hidden_units+960,\n                    out_features = hidden_units),\n            nn.BatchNorm1d(hidden_units),\n            nn.Dropout1d(p=0.7),\n            nn.LeakyReLU()\n            )\n        self.classifier2 = nn.Sequential(\n            nn.Linear(in_features=hidden_units,\n                    out_features = hidden_units),\n            nn.LogSoftmax(1)\n            )\n\n    \n    def forward(self, x):\n        x= self.conv_block_1(x)\n        #print(x.shape) \n        x= self.conv_block_2(x)\n        #print(x.shape)\n#       x= self.conv_block_3(x)\n        #print(x.shape)\n        x = self.classifier1(x)\n        x = self.classifier2(x)\n        #print(x.shape)\n        #print(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:31:59.243923Z","iopub.execute_input":"2024-06-09T18:31:59.244227Z","iopub.status.idle":"2024-06-09T18:31:59.258486Z","shell.execute_reply.started":"2024-06-09T18:31:59.244204Z","shell.execute_reply":"2024-06-09T18:31:59.257729Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Hiperparámetros modelo 3 ()\nmodel_3 = ModelV3(input_shape = 1,\n                hidden_units = 20,\n               output_shape = len(classNames))\n\nloss_function = nn.NLLLoss()\nepochs = 15\n\n#LEARNING RATE WARMUP\ninitial_lr = 0.01\nwarmup_iteration = 7\nwarmup_initial_lr = 1e-5\n\noptimizer = torch.optim.Adam(params = model_3.parameters(), lr=initial_lr, betas=(0.9,0.999))\nlr_scheduler = create_lr_scheduler_with_warmup(CosineAnnealingLR(optimizer, T_max=epochs-warmup_iteration),\n                                               warmup_start_value=warmup_initial_lr,\n                                               warmup_duration=warmup_iteration,\n                                               warmup_end_value=initial_lr)\n\n\n\nepoch_train_loss,epoch_train_acc,epoch_test_loss,epoch_test_acc = run_model(model_3, trainloader, testloader, loss_function, optimizer, accuracy_fn, device, epochs, scheduler = lr_scheduler)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:40:22.098207Z","iopub.execute_input":"2024-06-09T18:40:22.098583Z","iopub.status.idle":"2024-06-09T18:44:30.447572Z","shell.execute_reply.started":"2024-06-09T18:40:22.098553Z","shell.execute_reply":"2024-06-09T18:44:30.446578Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#GRAFICAR LOSS Y ACC\nplot_loss(epoch_train_loss,epoch_test_loss)\n\nplot_acc(epoch_train_acc,epoch_test_acc)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:44:57.642985Z","iopub.execute_input":"2024-06-09T18:44:57.643347Z","iopub.status.idle":"2024-06-09T18:44:58.210091Z","shell.execute_reply.started":"2024-06-09T18:44:57.643311Z","shell.execute_reply":"2024-06-09T18:44:58.209220Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#OBTENER RESULTADOS\nmodel_3_results = evaluate_model(model_3, testloader, loss_function, accuracy_fn, device)","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:45:03.488318Z","iopub.execute_input":"2024-06-09T18:45:03.488982Z","iopub.status.idle":"2024-06-09T18:45:05.874296Z","shell.execute_reply.started":"2024-06-09T18:45:03.488948Z","shell.execute_reply":"2024-06-09T18:45:05.873325Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **COMPARACIÓN DE LOS MODELOS**","metadata":{}},{"cell_type":"code","source":"compare_results = pd.DataFrame([model_1_results, model_2_results, model_3_results])\n\ngraph = compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\nplt.xlabel(\"Presición (%)\")\nplt.ylabel(\"Modelo\")\n\ngraph.set_xscale('linear')\ngraph.set_xlim([0, 100])","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:45:07.369831Z","iopub.execute_input":"2024-06-09T18:45:07.370178Z","iopub.status.idle":"2024-06-09T18:45:07.603789Z","shell.execute_reply.started":"2024-06-09T18:45:07.370150Z","shell.execute_reply":"2024-06-09T18:45:07.602692Z"},"trusted":true},"outputs":[],"execution_count":null}]}